/* eslint-disable no-console */
import { streamText, generateText } from 'ai'
import { withAuth } from '@/lib/auth/middleware'
import { prisma } from '@/lib/prisma/client'
import { DEFAULT_MODEL, AVAILABLE_MODELS } from '@/lib/constants/models'
import { getActiveApiKey, recordApiKeyUsage } from '@/lib/api-key-manager'
import { createOpenAI } from '@ai-sdk/openai'
import { getEncoding } from 'js-tiktoken'

const encoding = getEncoding('cl100k_base')

function countTokens(text: string): number {
  try {
    return encoding.encode(text).length
  } catch (e) {
    console.warn('Token counting failed:', e)
    return 0
  }
}

// ëª¨ë¸ë³„ ì œê³µì ë§¤í•‘ ì œê±° (AVAILABLE_MODELSì—ì„œ ì¡°íšŒ)

export const POST = withAuth(async (req: Request, userId: string) => {
  try {
    const { messages, conversationId, model = DEFAULT_MODEL, editMessageId, parentMessageId: clientParentMessageId } = await req.json()

    // ëª¨ë¸ ì„¤ì • ì¡°íšŒ (DB ìš°ì„ , ì—†ìœ¼ë©´ Static)
    let modelConfig: any = await prisma.model.findUnique({
      where: { id: model },
      include: { apiKey: true }
    })

    // DBì— ì—†ìœ¼ë©´ Static List í™•ì¸ (Legacy support)
    if (!modelConfig) {
      modelConfig = AVAILABLE_MODELS.find(m => m.id === model)
    }

    // ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
    if (!modelConfig) {
      return new Response('Invalid model', { status: 400 })
    }

    // ì œê³µì í™•ì¸
    const provider = modelConfig.provider

    // ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì—¬ë¶€ (DB ëª¨ë¸ì€ í•„ë“œ ì‚¬ìš©, TEXT/TEXT_VISIONì€ ê¸°ë³¸ true)
    const modelType = modelConfig.type ?? 'TEXT'
    const supportsStreaming = modelConfig.supportsStreaming ??
      (modelType === 'TEXT' || modelType === 'TEXT_VISION')

    // API í‚¤ ê°€ì ¸ì˜¤ê¸°
    let apiKeyId: string | null = null
    let activeKeyApiKey: string | null = null
    let activeKeyBaseUrl: string | null = null

    try {
      // 1. ëª¨ë¸ì— ì—°ê²°ëœ íŠ¹ì • API Keyê°€ ìˆìœ¼ë©´ ì‚¬ìš©
      if (modelConfig.apiKey) {
        if (!modelConfig.apiKey.isActive) {
          return new Response('ì—°ê²°ëœ API í‚¤ê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.', { status: 500 })
        }

        // Decrypt key
        const { decryptApiKey, deserializeEncryptedData } = await import('@/lib/crypto')
        const encryptedData = deserializeEncryptedData(modelConfig.apiKey.encryptedKeyJson)

        apiKeyId = modelConfig.apiKey.id
        activeKeyApiKey = decryptApiKey(encryptedData)
        activeKeyBaseUrl = modelConfig.apiKey.baseUrl || null
      } else {
        // 2. ì—†ìœ¼ë©´ ë¡œë“œë°¸ëŸ°ì‹±/ê¸°ë³¸ í‚¤ ì¡°íšŒ (ê¸°ì¡´ ë¡œì§)
        const activeKey = await getActiveApiKey(provider)
        if (!activeKey) {
          return new Response(`${provider} API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`, { status: 500 })
        }
        apiKeyId = activeKey.id
        activeKeyApiKey = activeKey.apiKey
        activeKeyBaseUrl = activeKey.baseUrl || null
      }
    } catch (error) {
      console.error('API í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨:', error)
      return new Response('API í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.', { status: 500 })
    }

    // ì‹¤ì œ API í˜¸ì¶œì— ì‚¬ìš©í•  ëª¨ë¸ ID
    // DB ëª¨ë¸ì¸ ê²½ìš° apiModelId ì‚¬ìš©, ì•„ë‹ˆë©´(static) model ID ê·¸ëŒ€ë¡œ ì‚¬ìš©
    const apiModelId = modelConfig.apiModelId || modelConfig.id

    // Save user message to database
    let userMessageId: string | null = null
    let parentMessageIdForAssistant: string | null = null
    let parentKeyForPath: string = 'root'

    if (conversationId) {
      const lastMessage = messages[messages.length - 1];
      let contentToSave = '';

      if (typeof lastMessage.content === 'string') {
        contentToSave = lastMessage.content;
      } else if (Array.isArray(lastMessage.content)) {
        // ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€(ì´ë¯¸ì§€ ë“±)ëŠ” JSON ë¬¸ìì—´ë¡œ ì €ì¥
        contentToSave = JSON.stringify(lastMessage.content);
      }

      if (editMessageId) {
        // ìˆ˜ì • ëª¨ë“œ: í˜•ì œ ë©”ì‹œì§€ ìƒì„± (ê°™ì€ ë¶€ëª¨ ë©”ì‹œì§€ ID ì‚¬ìš©)
        const originalMessage = await prisma.message.findUnique({
          where: { id: editMessageId }
        })

        if (originalMessage) {
          // ìƒˆ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í˜•ì œë¡œ ìƒì„± (ê°™ì€ parentMessageId)
          const newUserMessage = await prisma.message.create({
            data: {
              conversationId,
              role: 'user',
              content: contentToSave,
              parentMessageId: originalMessage.parentMessageId  // í˜•ì œ ê´€ê³„!
            }
          })
          userMessageId = newUserMessage.id
          parentMessageIdForAssistant = newUserMessage.id
          parentKeyForPath = originalMessage.parentMessageId || 'root'
          console.log(`[EditMessage] Created sibling message ${userMessageId} (parent: ${originalMessage.parentMessageId})`)
        }
      } else {
        // ì¼ë°˜ ëª¨ë“œ: ìƒˆ ë©”ì‹œì§€ ìƒì„± (íŠ¸ë¦¬ êµ¬ì¡°)
        const newUserMessage = await prisma.message.create({
          data: {
            conversationId,
            role: 'user',
            content: contentToSave,
            parentMessageId: clientParentMessageId || null  // í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì „ë‹¬ë°›ì€ ë¶€ëª¨ ID
          }
        })
        userMessageId = newUserMessage.id
        parentMessageIdForAssistant = newUserMessage.id
        parentKeyForPath = clientParentMessageId || 'root'
      }

      // v2.1: Update selectedPaths in Conversation
      if (userMessageId) {
        try {
          const conversation = await prisma.conversation.findUnique({
            where: { id: conversationId },
            select: { selectedPaths: true }
          })

          if (conversation) {
            const currentPaths = (conversation.selectedPaths as Record<string, string>) || {}
            await prisma.conversation.update({
              where: { id: conversationId },
              data: {
                selectedPaths: {
                  ...currentPaths,
                  [parentKeyForPath]: userMessageId
                }
              }
            })
          }
        } catch (pathError) {
          console.error('Failed to update selectedPaths:', pathError)
          // Non-critical error, continue
        }
      }
    }

    const startTime = Date.now()

    // Convert OpenAI 'image_url' format to Vercel AI SDK 'image' format
    const formattedMessages = messages.map((m: any) => {
      if (Array.isArray(m.content)) {
        return {
          ...m,
          content: m.content.map((c: any) => {
            if (c.type === 'image_url') {
              return {
                type: 'image',
                image: c.image_url.url
              }
            }
            return c
          })
        }
      }
      return m
    })

    const openaiProvider = createOpenAI({
      apiKey: activeKeyApiKey!,
      baseURL: activeKeyBaseUrl || undefined
    })
    /**
     * [ê²¬ê³ í•œ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜ - v1.5]
     * 
     * âš ï¸ ì¤‘ìš”: ì´ ë¡œì§ì€ ìƒˆë¡œê³ ì¹¨/í˜ì´ì§€ ì´ë™ ì‹œì—ë„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë³´ì¡´í•˜ê¸° ìœ„í•œ í•µì‹¬ ì½”ë“œì…ë‹ˆë‹¤!
     * 
     * ê¸°ì¡´ ë¬¸ì œ:
     * - ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ì—ë§Œ DBì— ì €ì¥ë˜ì–´, ì¤‘ê°„ì— ìƒˆë¡œê³ ì¹¨í•˜ë©´ ì‘ë‹µì´ ì˜êµ¬ ì†ì‹¤ë¨
     * - ìƒˆ ì±„íŒ… ìƒì„± í›„ router.push() ì‹œ í´ë¼ì´ì–¸íŠ¸ ìƒíƒœì™€ DBê°€ ë™ê¸°í™”ë˜ì§€ ì•ŠìŒ
     * 
     * í•´ê²° ë°©ë²•:
     * 1. ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì „ì— ë¹ˆ ë©”ì‹œì§€ë¥¼ DBì— ë¯¸ë¦¬ ìƒì„± (isStreaming: true)
     * 2. í´ë¼ì´ì–¸íŠ¸ê°€ ì–¸ì œë“  DBì—ì„œ í˜„ì¬ ì§„í–‰ ìƒíƒœë¥¼ ì¡°íšŒ ê°€ëŠ¥
     * 3. ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹œ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (isStreaming: false)
     * 
     * ğŸš« ì´ ë¡œì§ì„ ìˆ˜ì •í•  ë•Œ ì£¼ì˜ì‚¬í•­:
     * - prisma.message.createê°€ streamTextë³´ë‹¤ ë¨¼ì € í˜¸ì¶œë˜ì–´ì•¼ í•¨
     * - assistantMessageIdê°€ onFinishì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ í´ë¡œì €ì— ìº¡ì²˜ë¨
     * - isStreaming í•„ë“œëŠ” í”„ë¡ íŠ¸ì—”ë“œ polling ë¡œì§ê³¼ ì—°ë™ë¨
     */
    if (supportsStreaming) {
      let assistantContent = ''
      let assistantMessageId: string | null = null

      // [Step 1] ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì „ ë¹ˆ assistant ë©”ì‹œì§€ ë¯¸ë¦¬ ìƒì„±
      // - ìƒˆë¡œê³ ì¹¨ ì‹œ í”„ë¡ íŠ¸ì—”ë“œê°€ DBì—ì„œ ì´ ë©”ì‹œì§€ë¥¼ ì¡°íšŒí•˜ì—¬ ë³µêµ¬ ê°€ëŠ¥
      // - isStreaming: trueë¡œ ì„¤ì •í•˜ì—¬ "ì•„ì§ ì‘ë‹µ ì¤‘"ì„ì„ í‘œì‹œ
      if (conversationId) {
        const assistantMessage = await prisma.message.create({
          data: {
            conversationId,
            role: 'assistant',
            content: '',
            isStreaming: true,
            parentMessageId: parentMessageIdForAssistant  // ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶€ëª¨ë¡œ ì„¤ì •
          }
        })
        assistantMessageId = assistantMessage.id
      }

      const result = await streamText({
        model: openaiProvider(apiModelId),
        messages: formattedMessages,
        abortSignal: req.signal,
        onFinish: async ({ usage }) => {
          const responseTime = Date.now() - startTime

          // [Step 3] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (CREATE â†’ UPDATE íŒ¨í„´)
          // - isStreaming: falseë¡œ ë³€ê²½í•˜ì—¬ ì™„ë£Œ í‘œì‹œ
          // - í”„ë¡ íŠ¸ì—”ë“œ pollingì´ ì´ë¥¼ ê°ì§€í•˜ê³  polling ì¤‘ì§€
          if (assistantMessageId) {
            await prisma.message.update({
              where: { id: assistantMessageId },
              data: {
                content: assistantContent,
                isStreaming: false,
                tokens: usage?.totalTokens,
              }
            })
          }

          // Log token usage
          console.log('[TokenUsage] Stream finished. Usage:', usage)


          if (usage || assistantContent) {
            let promptTokens = usage ? (usage as any).promptTokens ?? 0 : 0
            let completionTokens = usage ? (usage as any).completionTokens ?? 0 : 0

            // Fallback: Calculate manually if usage is missing
            if (promptTokens === 0 && completionTokens === 0) {
              console.warn('[TokenUsage] Usage data missing, calculating manually...')
              // Calculate prompt tokens from messages
              const promptText = messages.map((m: any) => {
                if (typeof m.content === 'string') return m.content
                return m.content.filter((p: any) => p.type === 'text').map((p: any) => p.text).join('\n')
              }).join('\n')
              promptTokens = countTokens(promptText)
              // Calculate completion tokens from accumulated content
              completionTokens = countTokens(assistantContent)
            }

            await prisma.tokenUsage.create({
              data: {
                userId,
                model,
                promptTokens,
                completionTokens,
              }
            })
            console.log(`[TokenUsage] Saved: ${promptTokens}p, ${completionTokens}c (Source: ${usage ? 'Provider' : 'Manual'})`)
          } else {
            console.warn('[TokenUsage] No usage data available from stream')
          }

          // API í‚¤ ì‚¬ìš© ê¸°ë¡ (ì˜¤ë¥˜ ì—¬ë¶€ì— ê´€ê³„ì—†ì´ ê¸°ë¡)
          if (apiKeyId) {
            try {
              await recordApiKeyUsage({
                apiKeyId,
                endpoint: '/api/chat',
                model,
                tokens: usage?.totalTokens ?? 0,
                status: 'success',
                responseTime,
                errorMessage: undefined
              })
            } catch (usageError) {
              console.error('API í‚¤ ì‚¬ìš© ê¸°ë¡ ì‹¤íŒ¨:', usageError)
            }
          }
        },
        onChunk: ({ chunk }) => {
          // Accumulate the content for database storage
          if (chunk.type === 'text-delta') {
            assistantContent += (chunk as any).value ?? (chunk as any).text ?? ''
          }
        },
      })

      // ì‘ë‹µ í—¤ë”ì— ì‚¬ìš©ì ë©”ì‹œì§€ ID í¬í•¨
      const streamResponse = result.toTextStreamResponse()
      if (userMessageId) {
        const headers = new Headers(streamResponse.headers)
        headers.set('X-User-Message-Id', userMessageId)
        return new Response(streamResponse.body, {
          status: streamResponse.status,
          statusText: streamResponse.statusText,
          headers
        })
      }
      return streamResponse
    }

    // ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ì „ì²´ ì‘ë‹µ ì™„ë£Œ í›„ ì „ì†¡)
    const result = await generateText({
      model: openaiProvider(apiModelId),
      messages: formattedMessages,
      abortSignal: req.signal,
    })

    const responseTime = Date.now() - startTime
    const assistantContent = result.text
    const usage = result.usage

    console.log('[TokenUsage] Non-streaming usage:', usage)

    // Save assistant message to database
    if (conversationId) {
      await prisma.message.create({
        data: {
          conversationId,
          role: 'assistant',
          content: assistantContent,
          tokens: usage?.totalTokens,
        }
      })
    }

    // Log token usage
    // Log token usage
    if (usage || assistantContent) {
      let promptTokens = usage ? (usage as any).promptTokens ?? 0 : 0
      let completionTokens = usage ? (usage as any).completionTokens ?? 0 : 0

      // Fallback: Calculate manually if usage is missing
      if (promptTokens === 0 && completionTokens === 0) {
        console.warn('[TokenUsage] Usage data missing (non-stream), calculating manually...')
        // Calculate prompt tokens from messages
        const promptText = messages.map((m: any) => {
          if (typeof m.content === 'string') return m.content
          return m.content.filter((p: any) => p.type === 'text').map((p: any) => p.text).join('\n')
        }).join('\n')
        promptTokens = countTokens(promptText)
        completionTokens = countTokens(assistantContent)
      }

      await prisma.tokenUsage.create({
        data: {
          userId,
          model,
          promptTokens,
          completionTokens,
        }
      })
      console.log(`[TokenUsage] Saved: ${promptTokens}p, ${completionTokens}c (Source: ${usage ? 'Provider' : 'Manual'})`)
    } else {
      console.warn('[TokenUsage] No usage data available from non-streaming')
    }

    // API í‚¤ ì‚¬ìš© ê¸°ë¡
    if (apiKeyId) {
      try {
        await recordApiKeyUsage({
          apiKeyId,
          endpoint: '/api/chat',
          model,
          tokens: usage?.totalTokens ?? 0,
          status: 'success',
          responseTime,
          errorMessage: undefined
        })
      } catch (usageError) {
        console.error('API í‚¤ ì‚¬ìš© ê¸°ë¡ ì‹¤íŒ¨:', usageError)
      }
    }

    // JSON ì‘ë‹µ ë°˜í™˜ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
    return new Response(JSON.stringify({
      content: assistantContent,
      usage: usage ? {
        promptTokens: (usage as any).promptTokens,
        completionTokens: (usage as any).completionTokens,
        totalTokens: (usage as any).totalTokens,
      } : null,
    }), {
      headers: {
        'Content-Type': 'application/json',
      },
    })
  } catch (error) {
    console.error('Chat API error:', error)
    return new Response('Internal server error', { status: 500 })
  }
})

